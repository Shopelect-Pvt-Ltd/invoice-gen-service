# 1. Exporter
# It will take the pending request and export the csv  data to irn_table
# It will also check for the running job all the irn is generated by checking the s3_link!=Null
# It will also check for the requested download file if the irn is not present then it will create a new entry in irn_table

import time
from tendo import singleton
import psycopg2
import os
from dotenv import load_dotenv

load_dotenv()
import pandas as pd
import requests
import uuid
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(lineno)d - %(message)s'
)

postgres_host = os.getenv("PG_HOST")
postgres_db = os.getenv("PG_DATABASE")
postgres_user = os.getenv("PG_USER")
postgres_password = os.getenv("PG_PASSWORD")
postgres_port = os.getenv("PG_PORT")

conn = psycopg2.connect(
    host=postgres_host,
    database=postgres_db,
    port=postgres_port,
    user=postgres_user,
    password=postgres_password
)
cursor = conn.cursor()
logging.info("Postgres DB connected successfully.")

def updateJobStatus(status,reqid):
    if reqid is not None:
        query = "update invoice_gen_request set status='"+str(status)+"' where id='"+str(reqid)+"'"
        cursor.execute(query)
        conn.commit()
def getPendingJob():
    query = ("select id,user_id,type,source_file_url from invoice_gen_request where status='PENDING' order by "
             "created_at asc limit 1")
    cursor.execute(query)
    jobs_data = cursor.fetchall()
    columns = [
        'id',
        'user_id',
        'type',
        'source_file_url'
    ]
    data = [dict(zip(columns, row)) for row in jobs_data]
    return data

def downloadFile(url):
    filename = url.split('/')[-1]
    if not filename.endswith('.csv'):
        return None
    response = requests.get(url)
    for i in range(1, 3, 1):
        if response.status_code == 200:
            local_file_name = 'exporter_temp/' + filename
            with open(local_file_name, 'wb') as file:
                file.write(response.content)
            logging.info("Writing to the file is completed")
            return local_file_name
        time.sleep(2)
    return None

def exportToTable(filepath, reqid):
    try:
        cursor.execute("""
                CREATE TEMP TABLE invoice_gen_staging (
                    irn varchar,
                    reqid uuid
                );
            """)
        with open(filepath, 'r') as f:
            next(f)  # Skip the header row
            cursor.copy_expert("COPY invoice_gen_staging (irn, reqid) FROM STDIN WITH CSV", f)

        cursor.execute("""
                INSERT INTO invoice_gen (reqid, irn, s3_link)
                SELECT reqid, irn, NULL
                FROM invoice_gen_staging
                ON CONFLICT (irn) DO NOTHING
                RETURNING irn;
            """)
        rows_inserted = cursor.fetchall()

        if len(rows_inserted) == 0:
            updateJobStatus("PROCESSING",reqid)
        conn.commit()
        logging.info("Exported the csv data successfully..")
    except Exception as e:
        updateJobStatus( "FAILED",reqid)
        logging.info("Exception occurred in the exportToTable: " + str(e))

if __name__ == '__main__':
    try:
        me = singleton.SingleInstance()

        logging.info("===================================")
        logging.info("Fetching the pending jobs")
        jobs = getPendingJob()
        logging.info("No. of jobs found: " + str(len(jobs)))

        if jobs is not None and len(jobs) != 0:
            for job in jobs:
                logging.info("Job Details: " + str(job))
                source_file_url = job['source_file_url']
                reqid = job['id']
                local_file_path = downloadFile(source_file_url)
                if local_file_path is not None:
                    df = pd.read_csv(local_file_path)
                    df['reqid'] = reqid
                    df['reqid'] = df['reqid'].apply(lambda x: uuid.UUID(x))
                    os.remove(local_file_path)
                    df.to_csv(local_file_path, index=False)
                    exportToTable(local_file_path, reqid)
                    updateJobStatus("RUNNING",reqid)
                    logging.info("Exported irns to DB successfully")
                else:
                    updateJobStatus("FAILED",reqid)
                    logging.info("Error while inserting the irn file to DB")
        else:
            logging.info("No job")
        logging.info("===================================")
    except Exception as e:
        logging.info("Exception occurred in the main: " + str(e))
    finally:
        logging.info("Closing the DB connection")
        cursor.close()
        conn.close()
